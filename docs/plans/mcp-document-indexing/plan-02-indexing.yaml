conductor:
  default_agent: python-pro
  max_concurrency: 1

  quality_control:
    enabled: true
    review_agent: quality-control
    retry_on_red: 2

    agents:
      mode: intelligent
      max_agents: 2
      cache_ttl_seconds: 1800
      require_code_review: true
      explicit_list: []
      additional: []
      blocked: []

plan:
  metadata:
    phase_name: "Indexing & Deduplication Layer"
    parent_plan: "MCP Document Indexing & Semantic Retrieval"
    created: "2025-11-22"
    phase_number: 2
    target: "Implement document indexing pipeline with file hash tracking and batch embedding processing"
    tasks_count: 3
    estimated_total_time: "2h 15m"

  context:
    framework: "Python 3.14"
    architecture: "Indexing layer built on foundation (Task 1-6): token counting, chunking, embeddings, storage"
    test_framework: "pytest with 90% coverage target"
    dependencies_from_phase_1:
      - "Project structure initialized (Task 1)"
      - "Token counter with tiktoken (Task 2)"
      - "Markdown chunker with structure preservation (Task 3)"
      - "Ollama embedding client with retry logic (Task 4)"
      - "ChromaDB storage layer (Task 5)"
      - "Document and Chunk data models (Task 6)"
    other_context:
      - "Hash-based deduplication prevents re-indexing unchanged files"
      - "Batch processor handles up to 10 chunks with concurrent embedding"
      - "Target: Sub-second indexing for documents under 10KB"
      - "Goal: Zero data loss with atomic operations"
    expectations:
      - "Test-Driven Development: Write tests BEFORE implementation (red→green→refactor)"
      - "Google-style docstrings for all functions and classes"
      - "Mypy strict typing with --strict flag (no type: ignore comments)"
      - "Black formatting (line length 100)"
      - "Commit after each task completion"

  tasks:
    - task_number: 7
      type: "component"
      name: "Document Indexer Core"
      description: |
        Implement central document indexer that orchestrates the complete indexing workflow:
        chunking documents into logical pieces, generating embeddings for each chunk,
        and persisting them to ChromaDB with metadata tracking.

        The indexer combines MarkdownChunker (Task 3), OllamaEmbeddingClient (Task 4),
        and ChromaStore (Task 5) into a cohesive indexing pipeline with proper error
        handling and logging.

      agent: "python-pro"
      worktree_group: "chain-2"
      branch: "feature/mcp-document-indexing/chain-2"

      files:
        - "src/indexing/document_indexer.py"
        - "src/indexing/__init__.py"
        - "tests/unit/test_document_indexer.py"

      depends_on:
        - 2
        - 3
        - 4
        - 5
        - 6

      estimated_time: "1h"

      success_criteria:
        - "DocumentIndexer.index_document(file_path) chunks and indexes a single document"
        - "Returns IndexingResult with chunk count, embedding count, and status"
        - "Handles both plain text and markdown files"
        - "Gracefully recovers from failed embeddings (logs and skips bad chunks)"
        - "Mypy strict typing with no type: ignore comments"
        - "100% function/class docstrings in Google style"
        - "All public methods have type hints"
        - "No pylint warnings (disable only for legitimate reasons)"
        - "Indexes 10KB document in <2 seconds"
        - "Memory usage stays under 50MB for single document"
        - "Unit test coverage: 90% of source code"
        - "All code paths exercised including error cases"

      test_strategy:
        approach: "Test-Driven Development (TDD)"
        order: "Write tests first, then implementation"
        phases:
          - name: "Red Phase"
            description: "Write comprehensive tests that fail"
            tasks:
              - "Define IndexingResult dataclass"
              - "Write test for successful document indexing"
              - "Write test for file not found error handling"
              - "Write test for invalid file type handling"
              - "Write test for empty document handling"
              - "Write test for chunking integration"
              - "Write test for embedding generation"
              - "Write test for metadata preservation"
          - name: "Green Phase"
            description: "Implement minimal code to pass tests"
            tasks:
              - "Implement DocumentIndexer.__init__(chunker, embedding_client, storage)"
              - "Implement index_document(file_path) method"
              - "Implement file reading with encoding detection"
              - "Implement chunk creation and embedding"
              - "Implement storage persistence with metadata"
          - name: "Refactor Phase"
            description: "Improve code quality and maintainability"
            tasks:
              - "Extract file reading logic to helper method"
              - "Add comprehensive error logging"
              - "Optimize memory usage for large documents"
              - "Add docstring examples to all public methods"

      test_commands:
        - "python -m pytest tests/unit/test_document_indexer.py -v"
        - "python -m mypy src/indexing/document_indexer.py --strict"
        - "python -m black --check src/indexing/document_indexer.py"

      code_quality:
        python:
          mypy:
            command: "python -m mypy src/indexing/ --strict"
            exit_on_failure: true
          black:
            command: "python -m black src/indexing/ tests/unit/test_document_indexer.py"
            exit_on_failure: true
          isort:
            command: "python -m isort src/indexing/ tests/unit/test_document_indexer.py"
            exit_on_failure: true
          pytest:
            command: "python -m pytest tests/unit/test_document_indexer.py -v --cov=src/indexing/document_indexer --cov-report=term-missing"
            exit_on_failure: true
          full_quality_pipeline:
            command: |
              python -m black src/indexing/ tests/unit/test_document_indexer.py &&
              python -m isort src/indexing/ tests/unit/test_document_indexer.py &&
              python -m mypy src/indexing/document_indexer.py --strict &&
              python -m pytest tests/unit/test_document_indexer.py -v --cov=src/indexing/document_indexer --cov-report=term-missing:skip-covered
            exit_on_failure: true

      implementation_notes:
        key_classes:
          - name: "IndexingResult"
            type: "Pydantic BaseModel"
            fields:
              - "file_path: str"
              - "total_chunks: int"
              - "embedded_chunks: int"
              - "status: Literal['success', 'partial', 'failed']"
              - "error_message: Optional[str] = None"
              - "processing_time_seconds: float"
            usage: "Return value from index_document() method"

          - name: "DocumentIndexer"
            type: "Main orchestrator class"
            methods:
              - name: "__init__"
                params: "chunker, embedding_client, storage"
                purpose: "Initialize with dependency injection"
              - name: "index_document"
                params: "file_path: str, force_reindex: bool = False"
                returns: "IndexingResult"
                purpose: "Main entry point for indexing workflow"
              - name: "_read_file"
                params: "file_path: str"
                returns: "Tuple[str, str]"
                purpose: "Read file content with encoding detection"
              - name: "_create_chunks"
                params: "file_path: str, content: str"
                returns: "List[Chunk]"
                purpose: "Delegate to chunker for document splitting"
              - name: "_embed_chunks"
                params: "chunks: List[Chunk]"
                returns: "List[Tuple[Chunk, List[float]]]"
                purpose: "Generate embeddings via embedding client"
              - name: "_store_chunks"
                params: "file_path: str, chunks_with_embeddings"
                returns: "int"
                purpose: "Persist to ChromaDB via storage layer"

        design_patterns:
          - name: "Dependency Injection"
            description: "Pass chunker, embedding_client, and storage as constructor arguments"
            benefit: "Enables testing with mocks, easier to swap implementations"

          - name: "Error Recovery"
            description: "Continue indexing on individual chunk embedding failures"
            benefit: "Prevents total failure due to single bad chunk"

          - name: "Atomic Operations"
            description: "All-or-nothing chunk storage to prevent partial state"
            benefit: "Ensures database consistency"

        error_handling:
          - condition: "File not found"
            action: "Raise FileNotFoundError with helpful message"
            message: "File not found: {file_path}"

          - condition: "Invalid encoding"
            action: "Fall back to UTF-8 with error='replace'"
            message: "Could not decode file with detected encoding, using UTF-8 with replacement"

          - condition: "Chunk embedding fails"
            action: "Log warning and skip chunk, continue with others"
            message: "Failed to embed chunk {idx}: {error}"

          - condition: "Storage operation fails"
            action: "Rollback and raise with context"
            message: "Failed to store chunks: {error}"

      commit:
        type: "feat"
        message: |
          implement document indexer core with orchestration workflow

          - Add DocumentIndexer class orchestrating chunking→embedding→storage
          - Implement file reading with automatic encoding detection
          - Add IndexingResult dataclass for structured return values
          - Implement error recovery for individual chunk failures
          - Add comprehensive logging for debugging and monitoring
          - Include 100% docstring coverage with usage examples

        files:
          - "src/indexing/__init__.py"
          - "src/indexing/document_indexer.py"
          - "tests/unit/test_document_indexer.py"

      validation_checklist:
        - "Tests written first and failing (red phase)"
        - "Implementation passes all tests (green phase)"
        - "Code refactored for clarity (refactor phase)"
        - "All files pass mypy --strict"
        - "All files formatted with black"
        - "Coverage report shows >90% coverage"
        - "No TODO or FIXME comments without issue numbers"
        - "All docstrings follow Google style"
        - "Commit message follows conventional commits"
        - "Git diff reviewed for accidental changes"

    - task_number: 8
      type: "component"
      name: "File Hash Tracking System"
      description: |
        Implement persistent file hash tracking to prevent re-indexing of unchanged documents.
        This system stores SHA-256 hashes of previously indexed files and compares with
        current content to determine if re-indexing is needed.

        Maintains a SQLite hash registry stored alongside ChromaDB data, enabling smart
        deduplication at the application level with configurable strategies (update, skip, force).

      agent: "python-pro"
      worktree_group: "chain-2"
      branch: "feature/mcp-document-indexing/chain-2"

      files:
        - "src/indexing/hash_registry.py"
        - "src/indexing/file_hasher.py"
        - "tests/unit/test_hash_registry.py"
        - "tests/unit/test_file_hasher.py"

      depends_on:
        - 1
        - 6
        - 7

      estimated_time: "50m"

      success_criteria:
        - "FileHasher.calculate_hash(file_path) returns SHA-256 hash string"
        - "Hash changes when file content changes"
        - "Hash remains identical for same content (deterministic)"
        - "HashRegistry stores and retrieves hashes from persistent storage"
        - "get_previous_hash(file_path) returns None for new files"
        - "is_file_changed(file_path) returns boolean accurately"
        - "Registry handles missing files gracefully"
        - "All functions have Google-style docstrings"
        - "Mypy strict typing enforced"
        - "No type: ignore comments allowed"
        - "File hashing for 1MB file completes in <500ms"
        - "Hash registry queries execute in <10ms"
        - "Memory efficient streaming for large files"

      test_strategy:
        approach: "Test-Driven Development (TDD)"
        order: "Tests before implementation"
        phases:
          - name: "Red Phase"
            description: "Write comprehensive tests that fail"
            tasks:
              - "Test FileHasher.calculate_hash() returns consistent hash"
              - "Test hash changes with file content modification"
              - "Test hash computation for various file sizes"
              - "Test HashRegistry initialization"
              - "Test storing hash in registry"
              - "Test retrieving hash from registry"
              - "Test is_file_changed() detection"
              - "Test handling of missing files"
              - "Test registry persistence across sessions"

          - name: "Green Phase"
            description: "Implement minimal code to pass tests"
            tasks:
              - "Implement FileHasher class with calculate_hash() method"
              - "Use hashlib.sha256() for stable hashing"
              - "Implement streaming for memory efficiency"
              - "Create HashRegistry class with SQLite backend"
              - "Implement store_hash() and get_hash() methods"
              - "Implement is_file_changed() detection logic"

          - name: "Refactor Phase"
            description: "Improve maintainability and performance"
            tasks:
              - "Extract database schema to separate constant"
              - "Add logging for hash operations"
              - "Optimize query performance with indexes"
              - "Add connection pooling for better performance"

      test_commands:
        - "python -m pytest tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py -v"
        - "python -m mypy src/indexing/file_hasher.py src/indexing/hash_registry.py --strict"

      code_quality:
        python:
          mypy:
            command: "python -m mypy src/indexing/file_hasher.py src/indexing/hash_registry.py --strict"
            exit_on_failure: true
          black:
            command: "python -m black src/indexing/file_hasher.py src/indexing/hash_registry.py tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py"
            exit_on_failure: true
          isort:
            command: "python -m isort src/indexing/file_hasher.py src/indexing/hash_registry.py tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py"
            exit_on_failure: true
          pytest:
            command: "python -m pytest tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py -v --cov=src/indexing/file_hasher --cov=src/indexing/hash_registry --cov-report=term-missing"
            exit_on_failure: true
          full_quality_pipeline:
            command: |
              python -m black src/indexing/file_hasher.py src/indexing/hash_registry.py tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py &&
              python -m isort src/indexing/file_hasher.py src/indexing/hash_registry.py tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py &&
              python -m mypy src/indexing/file_hasher.py src/indexing/hash_registry.py --strict &&
              python -m pytest tests/unit/test_file_hasher.py tests/unit/test_hash_registry.py -v --cov=src/indexing/file_hasher --cov=src/indexing/hash_registry --cov-report=term-missing:skip-covered
            exit_on_failure: true

      implementation_notes:
        key_classes:
          - name: "FileHasher"
            type: "Utility class for hash computation"
            methods:
              - name: "calculate_hash"
                params: "file_path: str, chunk_size: int = 65536"
                returns: "str"
                purpose: "Compute SHA-256 hash with streaming for memory efficiency"
              - name: "_read_file_chunks"
                params: "file_path: str, chunk_size: int"
                returns: "Iterator[bytes]"
                purpose: "Yield file chunks for streaming hash computation"
            docstring_example: |
              """
              hasher = FileHasher()
              hash_value = hasher.calculate_hash('document.md')
              # Returns: 'a3f5c8...' (SHA-256 hex string)
              """

          - name: "HashRegistry"
            type: "Persistent hash storage with SQLite backend"
            methods:
              - name: "__init__"
                params: "db_path: str"
                purpose: "Initialize registry with SQLite database"
              - name: "store_hash"
                params: "file_path: str, hash_value: str"
                returns: "None"
                purpose: "Store or update file hash in registry"
              - name: "get_hash"
                params: "file_path: str"
                returns: "Optional[str]"
                purpose: "Retrieve stored hash for file"
              - name: "is_file_changed"
                params: "file_path: str"
                returns: "bool"
                purpose: "Check if file content has changed since last indexing"
              - name: "delete_hash"
                params: "file_path: str"
                returns: "None"
                purpose: "Remove hash entry for deleted file"
            schema: |
              CREATE TABLE file_hashes (
                file_path TEXT PRIMARY KEY,
                hash_value TEXT NOT NULL,
                indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
              );
              CREATE INDEX idx_file_hashes_path ON file_hashes(file_path);

        design_patterns:
          - name: "Streaming Hash Computation"
            description: "Read file in chunks to avoid loading entire file in memory"
            benefit: "Enables hashing of arbitrarily large files"

          - name: "Registry Pattern"
            description: "Centralized persistent storage for hash state"
            benefit: "Single source of truth for deduplication state"

        error_handling:
          - condition: "File does not exist"
            action: "Raise FileNotFoundError"
            message: "Cannot hash non-existent file: {file_path}"

          - condition: "Permission denied reading file"
            action: "Raise PermissionError with context"
            message: "Permission denied reading file: {file_path}"

          - condition: "Hash registry database corrupted"
            action: "Log error and reinitialize database"
            message: "Hash registry corrupted, reinitializing database"

      commit:
        type: "feat"
        message: |
          implement file hash tracking for deduplication

          - Add FileHasher class for SHA-256 computation with streaming
          - Implement HashRegistry with persistent SQLite storage
          - Add is_file_changed() detection for smart re-indexing
          - Include database schema with indexed lookups
          - Add comprehensive error handling and logging
          - Cover edge cases: missing files, permission errors, database corruption

        files:
          - "src/indexing/file_hasher.py"
          - "src/indexing/hash_registry.py"
          - "tests/unit/test_file_hasher.py"
          - "tests/unit/test_hash_registry.py"

      validation_checklist:
        - "All hash computations produce consistent results"
        - "Hash changes detected accurately for modified files"
        - "Database persists hashes across application restarts"
        - "Mypy strict mode passes without type: ignore"
        - "Coverage >90% on both modules"
        - "Performance benchmarks met (hash <500ms, query <10ms)"
        - "All docstrings complete with examples"
        - "Error messages helpful for debugging"
        - "No SQL injection vulnerabilities"
        - "Commit message clear and descriptive"

    - task_number: 9
      type: "component"
      name: "Batch Embedding Processor"
      description: |
        Implement efficient batch processing for document chunks with concurrent embedding
        generation. Processes up to 10 chunks at once through the Ollama embedding service
        with proper queue management, progress tracking, and error recovery.

        This component maximizes GPU utilization by batching requests while maintaining
        memory constraints and providing real-time progress updates for long-running operations.

      agent: "python-pro"
      worktree_group: "chain-2"
      branch: "feature/mcp-document-indexing/chain-2"

      files:
        - "src/indexing/batch_processor.py"
        - "src/indexing/__init__.py"
        - "tests/unit/test_batch_processor.py"

      depends_on:
        - 4
        - 7
        - 8

      estimated_time: "45m"

      success_criteria:
        - "BatchProcessor.process_batch(chunks, max_batch_size=10) processes up to 10 chunks concurrently"
        - "Returns BatchResult with success/failure counts and processing time"
        - "Chunks are processed in order (deterministic output)"
        - "Individual chunk failures do not block other chunks"
        - "Progress callback invoked after each batch completion"
        - "Handles empty chunk list gracefully"
        - "Google-style docstrings on all public methods"
        - "Mypy strict typing with no type: ignore"
        - "All async operations properly typed"
        - "Process 10 chunks (100 tokens each) in <3 seconds"
        - "Memory usage stays under 100MB during batch processing"
        - "Queue management prevents memory leaks"
        - "No blocking I/O on main thread"

      test_strategy:
        approach: "Test-Driven Development (TDD) with async testing"
        order: "Write async tests first, then implementation"
        phases:
          - name: "Red Phase"
            description: "Write comprehensive async tests"
            tasks:
              - "Test batch processing with 5 chunks"
              - "Test batch processing with 10 chunks (max)"
              - "Test processing with more than 10 chunks (multiple batches)"
              - "Test individual chunk failure handling"
              - "Test progress callback invocation"
              - "Test empty chunk list handling"
              - "Test BatchResult structure"
              - "Test concurrent embedding requests"

          - name: "Green Phase"
            description: "Implement minimal async code"
            tasks:
              - "Implement BatchProcessor class"
              - "Implement process_batch() async method"
              - "Implement chunk queue with size limit"
              - "Integrate with OllamaEmbeddingClient"
              - "Implement progress tracking"
              - "Implement error recovery per chunk"

          - name: "Refactor Phase"
            description: "Optimize async patterns"
            tasks:
              - "Optimize concurrent requests with asyncio.gather()"
              - "Add timeout handling for stalled requests"
              - "Improve error messages with context"
              - "Add metrics for monitoring performance"

      test_commands:
        - "python -m pytest tests/unit/test_batch_processor.py -v -k 'batch'"
        - "python -m mypy src/indexing/batch_processor.py --strict"

      code_quality:
        python:
          mypy:
            command: "python -m mypy src/indexing/batch_processor.py --strict"
            exit_on_failure: true
          black:
            command: "python -m black src/indexing/batch_processor.py tests/unit/test_batch_processor.py"
            exit_on_failure: true
          isort:
            command: "python -m isort src/indexing/batch_processor.py tests/unit/test_batch_processor.py"
            exit_on_failure: true
          pytest:
            command: "python -m pytest tests/unit/test_batch_processor.py -v --cov=src/indexing/batch_processor --cov-report=term-missing"
            exit_on_failure: true
          full_quality_pipeline:
            command: |
              python -m black src/indexing/batch_processor.py tests/unit/test_batch_processor.py &&
              python -m isort src/indexing/batch_processor.py tests/unit/test_batch_processor.py &&
              python -m mypy src/indexing/batch_processor.py --strict &&
              python -m pytest tests/unit/test_batch_processor.py -v --cov=src/indexing/batch_processor --cov-report=term-missing:skip-covered
            exit_on_failure: true

      implementation_notes:
        key_classes:
          - name: "BatchResult"
            type: "Pydantic BaseModel for return value"
            fields:
              - "total_chunks: int"
              - "successful_chunks: int"
              - "failed_chunks: int"
              - "processing_time_seconds: float"
              - "embeddings: List[List[float]]"
              - "errors: List[str]"
            usage: "Return value from process_batch()"

          - name: "BatchProcessor"
            type: "Async batch processor for embeddings"
            constructor_params:
              - "embedding_client: OllamaEmbeddingClient"
              - "max_batch_size: int = 10"
              - "timeout_seconds: float = 30.0"
            methods:
              - name: "process_batch"
                signature: "async def process_batch(chunks: List[Chunk], progress_callback: Optional[Callable] = None) -> BatchResult"
                purpose: "Process chunks concurrently up to max_batch_size"
              - name: "_chunk_chunks"
                signature: "def _chunk_chunks(chunks: List[Chunk]) -> Iterator[List[Chunk]]"
                purpose: "Split chunks into batches of max_batch_size"
              - name: "_process_sub_batch"
                signature: "async def _process_sub_batch(batch: List[Chunk]) -> Tuple[List[List[float]], List[str]]"
                purpose: "Process single batch of chunks, return embeddings and errors"
            docstring_example: |
              """
              processor = BatchProcessor(embedding_client)
              chunks = [chunk1, chunk2, ..., chunk10]

              async def progress(current: int, total: int):
                  print(f"Processed {current}/{total}")

              result = await processor.process_batch(chunks, progress)
              print(f"Success: {result.successful_chunks}/{result.total_chunks}")
              """

        async_patterns:
          - name: "Concurrent Embedding Generation"
            description: "Use asyncio.gather() to process multiple chunks in parallel"
            code_pattern: |
              async def _process_sub_batch(self, batch: List[Chunk]) -> Tuple[List[List[float]], List[str]]:
                  tasks = [
                      self.embedding_client.embed_async(chunk.text)
                      for chunk in batch
                  ]
                  results = await asyncio.gather(*tasks, return_exceptions=True)
                  embeddings = [r for r in results if not isinstance(r, Exception)]
                  errors = [str(r) for r in results if isinstance(r, Exception)]
                  return embeddings, errors

          - name: "Progress Tracking"
            description: "Invoke callback after each sub-batch completes"
            code_pattern: |
              for i, batch in enumerate(self._chunk_chunks(chunks)):
                  embeddings, errors = await self._process_sub_batch(batch)
                  if progress_callback:
                      await progress_callback(i * max_batch_size, len(chunks))

        design_patterns:
          - name: "Batch Queue Pattern"
            description: "Queue chunks in batches, process with bounded concurrency"
            benefit: "Prevents memory exhaustion from processing too many chunks simultaneously"

          - name: "Callback Progress Reporting"
            description: "Optional progress callback for UI/logging integration"
            benefit: "Long-running operations can report progress to caller"

          - name: "Error Isolation"
            description: "Individual chunk failures captured but don't block batch"
            benefit: "Robust processing even with occasional transient failures"

        concurrency_considerations:
          - "Max 10 concurrent embedding requests to Ollama service"
          - "Timeout after 30 seconds for stuck requests"
          - "Use asyncio.gather() with return_exceptions=True"
          - "Preserve chunk order in output for deterministic results"
          - "Lock-free design using asyncio primitives only"

        error_handling:
          - condition: "Embedding service timeout"
            action: "Catch asyncio.TimeoutError, add to errors list"
            message: "Embedding request timed out after 30s for chunk {idx}"

          - condition: "Embedding service unavailable"
            action: "Propagate with context about retry strategy"
            message: "Embedding service unavailable: {error}"

          - condition: "Empty chunk list"
            action: "Return BatchResult with all zeros"
            message: "No chunks to process"

      commit:
        type: "feat"
        message: |
          implement batch embedding processor with concurrent requests

          - Add BatchProcessor class for concurrent embedding generation
          - Implement process_batch() async method with max_batch_size=10
          - Add queue management to prevent memory exhaustion
          - Include progress callback for long-running operations
          - Implement error recovery: individual failures don't block batch
          - Add BatchResult dataclass for structured return values
          - Include timeout handling and graceful degradation

        files:
          - "src/indexing/batch_processor.py"
          - "src/indexing/__init__.py"
          - "tests/unit/test_batch_processor.py"

      validation_checklist:
        - "All async tests pass with pytest-asyncio"
        - "Concurrent processing verified with timing assertions"
        - "Error handling tested for all failure modes"
        - "Progress callback invoked at expected intervals"
        - "Memory usage stays under 100MB during batch ops"
        - "Chunk order preserved in output"
        - "Mypy strict mode passes without type: ignore"
        - "Coverage >90% including async paths"
        - "No race conditions in concurrent operations"
        - "Timeout handling prevents indefinite blocking"
        - "All docstrings complete with examples"
        - "Commit message references all changes"

  integration_notes:
    phase_dependencies:
      - "Phase 1 (Tasks 1-6) must be completed before phase 2"
      - "Task 7 uses components from Tasks 2-6"
      - "Task 8 extends Task 7 with deduplication"
      - "Task 9 optimizes Task 7 with batch processing"

    worktree_strategy:
      - "Use single worktree for chain-2 (sequential execution)"
      - "Check out from feature/mcp-document-indexing/chain-1 after Phase 1 completion"
      - "Tag commits as task-7-complete, task-8-complete, task-9-complete"
      - "Merge back to main after all 3 tasks pass QC"

    code_review_focus:
      - "Dependency injection in DocumentIndexer"
      - "Error handling gracefully across all layers"
      - "Type safety with mypy --strict"
      - "Async patterns in batch processor"
      - "Database schema and SQL safety in hash registry"
      - "Performance characteristics match targets"

  execution_summary:
    total_estimated_time: "2h 15m"
    breakdown:
      - "Task 7 (DocumentIndexer Core): 1h"
      - "Task 8 (Hash Tracking): 50m"
      - "Task 9 (Batch Processor): 45m"
    qc_overhead: "15-20m per task"
    contingency: "15m (unplanned issues)"
    total_with_qc: "3h 30m"

  success_metrics:
    code_quality:
      - "100% of functions have docstrings"
      - "0 mypy errors with --strict flag"
      - "90%+ test coverage on all modules"
      - "0 pylint warnings (legitimate disables acceptable)"
      - "All files formatted consistently with black"
    functionality:
      - "All success criteria met for each task"
      - "Integration tests verify components work together"
      - "No regressions in Phase 1 components"
    performance:
      - "Document indexing <2s for 10KB files"
      - "File hashing <500ms for 1MB files"
      - "Batch processing <3s for 10x100-token chunks"
    reliability:
      - "Zero data loss in atomicity verification"
      - "Error recovery tested for all failure modes"
      - "Hash registry consistency verified"
